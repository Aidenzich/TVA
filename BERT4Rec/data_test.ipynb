{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiden/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataloaders import BertDataloader\n",
    "from templates import set_template\n",
    "from options import args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_template(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmpdatasets import dataset_factory\n",
    "dataset = dataset_factory(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already preprocessed. Skip preprocessing\n",
      "Negatives samples exist. Loading.\n",
      "Negatives samples exist. Loading.\n"
     ]
    }
   ],
   "source": [
    "from dataloaders import dataloader_factory\n",
    "train_loader, val_loader, test_loader = dataloader_factory(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BERTModel\n",
    "\n",
    "mymodel = BERTModel(\n",
    "    hidden_size=256,\n",
    "    num_items=train_loader.dataset.num_items,     # item 的數量\n",
    "    n_layers=2,\n",
    "    dropout=0,\n",
    "    heads=8,\n",
    "    max_len=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/aiden/Documents/recsys-sequence-playground/BERT4Rec/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | bert | BERT   | 2.4 M \n",
      "1 | out  | Linear | 771 K \n",
      "--------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.580    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiden/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [127,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [179,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    722\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    807\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    808\u001b[0m )\n\u001b[0;32m--> 809\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    811\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1234\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1236\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1321\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1343\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1345\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1411\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1411\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1413\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:154\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 154\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    156\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:127\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    128\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:222\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mvalidation_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    224\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1763\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1765\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:344\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m--> 344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/recsys-sequence-playground/BERT4Rec/model.py:52\u001b[0m, in \u001b[0;36mBERTModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m seqs, candidates, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> 52\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(seqs)                 \u001b[39m# B x T x V\u001b[39;00m\n\u001b[1;32m     53\u001b[0m scores \u001b[39m=\u001b[39m scores[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]                   \u001b[39m# B x V\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/recsys-sequence-playground/BERT4Rec/model.py:34\u001b[0m, in \u001b[0;36mBERTModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 34\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(x)\n\u001b[1;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/recsys-sequence-playground/BERT4Rec/model.py:89\u001b[0m, in \u001b[0;36mBERT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m transformer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks:\n\u001b[0;32m---> 89\u001b[0m     x \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mforward(x, mask)\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/recsys-sequence-playground/BERT4Rec/model.py:167\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask):\n\u001b[0;32m--> 167\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_sublayer(x, \u001b[39mlambda\u001b[39;49;00m _x: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention\u001b[39m.\u001b[39;49mforward(_x, _x, _x, mask\u001b[39m=\u001b[39;49mmask))\n\u001b[1;32m    168\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_sublayer(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/recsys-sequence-playground/BERT4Rec/model.py:277\u001b[0m, in \u001b[0;36mSublayerConnection.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mApply residual connection to any sublayer with the same size.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 277\u001b[0m \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sublayer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(x)))\n",
      "File \u001b[0;32m~/Documents/recsys-sequence-playground/BERT4Rec/model.py:167\u001b[0m, in \u001b[0;36mTransformerBlock.forward.<locals>.<lambda>\u001b[0;34m(_x)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask):\n\u001b[0;32m--> 167\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_sublayer(x, \u001b[39mlambda\u001b[39;00m _x: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention\u001b[39m.\u001b[39;49mforward(_x, _x, _x, mask\u001b[39m=\u001b[39;49mmask))\n\u001b[1;32m    168\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_sublayer(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward)\n",
      "File \u001b[0;32m~/Documents/recsys-sequence-playground/BERT4Rec/model.py:195\u001b[0m, in \u001b[0;36mMultiHeadedAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m# 1) Do all the linear projections in batch from d_model => h x d_k\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m query, key, value \u001b[39m=\u001b[39m [l(x)\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_k)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    196\u001b[0m                      \u001b[39mfor\u001b[39;00m l, x \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_layers, (query, key, value))]\n\u001b[1;32m    198\u001b[0m \u001b[39m# 2) Apply attention on all the projected vectors in batch.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/recsys-sequence-playground/BERT4Rec/model.py:195\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m# 1) Do all the linear projections in batch from d_model => h x d_k\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m query, key, value \u001b[39m=\u001b[39m [l(x)\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_k)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    196\u001b[0m                      \u001b[39mfor\u001b[39;00m l, x \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_layers, (query, key, value))]\n\u001b[1;32m    198\u001b[0m \u001b[39m# 2) Apply attention on all the projected vectors in batch.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/aiden/Documents/recsys-sequence-playground/BERT4Rec/data_test.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bikmmy/home/aiden/Documents/recsys-sequence-playground/BERT4Rec/data_test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bikmmy/home/aiden/Documents/recsys-sequence-playground/BERT4Rec/data_test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(limit_train_batches\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bikmmy/home/aiden/Documents/recsys-sequence-playground/BERT4Rec/data_test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(mymodel, train_loader, val_loader)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 768\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    769\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    770\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:736\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mreconciliate_processes(traceback\u001b[39m.\u001b[39mformat_exc())\n\u001b[1;32m    735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_exception\u001b[39m\u001b[39m\"\u001b[39m, exception)\n\u001b[0;32m--> 736\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_teardown()\n\u001b[1;32m    737\u001b[0m \u001b[39m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstage \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1298\u001b[0m, in \u001b[0;36mTrainer._teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[39m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[39mCallback; those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_dispatch(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1298\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mteardown()\n\u001b[1;32m   1299\u001b[0m loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_active_loop\n\u001b[1;32m   1300\u001b[0m \u001b[39m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/strategies/single_device.py:96\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mteardown()\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     95\u001b[0m     \u001b[39m# GPU teardown\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mcpu()\n\u001b[1;32m     97\u001b[0m     \u001b[39m# clean up memory\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py:147\u001b[0m, in \u001b[0;36mDeviceDtypeModuleMixin.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m    Module: self\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__update_properties(device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 147\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcpu()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:718\u001b[0m, in \u001b[0;36mModule.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcpu\u001b[39m(\u001b[39mself\u001b[39m: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    710\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \n\u001b[1;32m    712\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcpu())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:718\u001b[0m, in \u001b[0;36mModule.cpu.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcpu\u001b[39m(\u001b[39mself\u001b[39m: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    710\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \n\u001b[1;32m    712\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcpu())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=10, gpus=1)\n",
    "trainer.fit(mymodel, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70668f4c7653747fd59edaf7d5b50b4bbda7f2bf42a89fa403a332c71a486cbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
